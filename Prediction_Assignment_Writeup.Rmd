---
title: "Prediction Assignment Writeup (ml)"
author: "shubham"
date: "October 7, 2020"
output:
  pdf_document: default
  html_document: default
---

The goal of this project is to predict the manner in which people did the exercise.

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

## Pre-processing Data
Several columns of the raw data set have string contaning nothing, so we delete those columns first, and we also delete the first 7 columns: X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window. These features are obviously not related to predict the outcome.

```{r}
library(ggplot2)
library(caret)
library(fscaret)
library(randomForest)
library(e1071)

training <- read.table("pml-training.csv", sep = ",", header = TRUE)
testing <- read.table("pml-testing.csv", sep = ",", header = TRUE)


```


##Splitting the data and selecting features
 
I set the seed to make this analysis reproducible.
We need to split the original training set into our training set and a validation set.

Many columns of the data set contain the same value accros the lines. These “near-zero variance predictors”" bring almost no information to our model and will make computing unnecessarily longer. Others are entirely filled with NA values. Finnaly, the six first variables do not concern fitness motions whatsoever. They also need to be remove before we start fitting our model.

```{r}

set.seed(333)



inTrain <- createDataPartition(y=training$classe, p=0.7, list=F)
training1 <- training[inTrain, ]
training2 <- training[-inTrain, ]



#removing near-zero variance predictors
nzv <- nearZeroVar(training)
training1 <- training1[, -nzv]
training2 <- training2[, -nzv]
#removing predictors with NA values
training1 <- training1[, colSums(is.na(training1)) == 0]
training2 <- training2[, colSums(is.na(training2)) == 0]
#removing columns unfit for prediction (ID, user_name, raw_timestamp_part_1 etc ...)
training1 <- training1[, -(1:5)]
training2 <- training2[, -(1:5)]

```
##Selecting a model
We chose to fit a random forest model. This model provided the most accurate results all along the machine learning course.The cross-validation is set to draw a subset of the data three different times.


```{r}
mod1 <- train(classe~. , method ="rf", data = training1, verbose = TRUE, trControl = trainControl(method="cv"), number = 3)
pred1 <- predict(mod1, training1)
confusionMatrix(pred1, training1$classe)

#We get a very high accuracy of 99% but we still need to know how this model performs against the test set before expressing a conclusion.

pred12 <- predict(mod1, training2)
confusionMatrix(pred12, training2$classe)
##Testing the model
##We apply to the final test set the same features selection method that we use for the training set

testing <- testing[, colSums(is.na(testing)) == 0]
testing <- testing[, -(1:5)]
nzvt <- nearZeroVar(testing)
testing <- testing[, -nzvt]
#We test the random forest model on the test set

pred13 <- predict(mod1, testing)
pred13

```

